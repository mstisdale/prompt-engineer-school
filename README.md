# prompt-engineer-school
Educational resources on prompt engineering
## How to Use and Adjust Temperature Settings for Large Language Models

Large language models (LLMs) are neural networks with billions of parameters that can generate natural language texts for various tasks and domains. They are trained on massive amounts of unlabeled text using self-supervised learning, which means they learn to predict the next word or token in a sequence based on the previous context. LLMs have shown impressive performance and versatility across many natural language processing (NLP) applications, such as text summarization, question answering, text generation, and more.

However, LLMs are not perfect and sometimes produce texts that are nonsensical, irrelevant, or even harmful. One way to control the quality and diversity of the generated texts is to use a temperature parameter that affects the randomness of the predictions. In this blog post, we will explain what temperature is, how it works, and how to use and adjust it for different purposes and scenarios.

## What is Temperature?

Temperature is a hyperparameter of LLMs that controls the randomness of predictions by scaling the logits before applying softmax. Logits are the raw outputs of the neural network before they are normalized into probabilities. Softmax is a function that converts logits into probabilities that sum up to one. The temperature parameter affects how much the logits are divided by before computing the softmax.

For example, suppose we have a sentence "The sky is" and we want to generate the next word using an LLM. The LLM will produce a vector of logits for each possible word in the vocabulary, such as [2.3, 1.5, -0.7, ...] for [blue, red, green, ...]. Then, it will apply softmax to get the probabilities for each word, such as [0.5, 0.2, 0.01, ...]. The LLM will then sample a word from this probability distribution, such as blue with 50% chance.

Now, if we apply a temperature parameter T to the logits before softmax, we get a new vector of logits [2.3/T, 1.5/T, -0.7/T, ...]. This will affect the softmax probabilities in different ways depending on the value of T:

- T = 0: This is equivalent to taking the maximum of the logits and setting it to one and the rest to zero. This means the LLM will deterministically choose the most probable word according to its logits, regardless of the softmax probabilities. In our example, this would be blue with 100% chance.
- T = 1: This is equivalent to leaving the logits unchanged and applying softmax normally. This means the LLM will sample from the original probability distribution produced by its logits. In our example, this would be blue with 50% chance, red with 20% chance, green with 1% chance, etc.
- T > 1: This is equivalent to dividing the logits by a positive number larger than one and applying softmax. This means the LLM will smooth out the probability distribution and make it more uniform. This increases the chances of sampling low-probability words and decreases the chances of sampling high-probability words. In our example, this would make blue less likely and green more likely than before.
- T < 1: This is equivalent to dividing the logits by a positive number smaller than one and applying softmax. This means the LLM will sharpen the probability distribution and make it more peaked. This decreases the chances of sampling low-probability words and increases the chances of sampling high-probability words. In our example, this would make blue more likely and green less likely than before.

## How to Use and Adjust Temperature?

The choice of temperature depends on what kind of texts we want to generate from an LLM and what trade-offs we are willing to make between quality and diversity. Generally speaking:

- High temperature (T > 1): This will produce more diverse and creative texts, but also more nonsensical and irrelevant ones. This can be useful for tasks that require exploration and novelty, such as story generation or artistic expression.
- Low temperature (T < 1): This will produce more coherent and relevant texts, but also more predictable and boring ones. This can be useful for tasks that require accuracy and consistency, such as summarization or factual reporting.
- Medium temperature (T = 1): This will produce a balance between quality and diversity, but also a mix of good and bad texts. This can be useful for tasks that require some degree of flexibility and variation, such as dialogue or question answering.

The optimal value of temperature may vary depending on the task, domain, data, model, and user preference. There is no one-size-fits-all solution for choosing temperature; it requires experimentation and evaluation to find the best setting for each case.

One way to adjust temperature is to use a fixed value for all texts generated by an LLM. For example, we can set T = 0.8 for summarization or T = 1.2 for story generation.

Another way to adjust temperature is to use a dynamic value that changes depending on some factors or conditions related to the text generation process. For example:

- We can use a higher temperature at the beginning of a text generation session to encourage exploration and creativity; then gradually lower it as we generate more texts to ensure coherence and relevance.
- We can use a higher temperature when generating texts for topics or domains that are unfamiliar or underrepresented in the training data; then lower it when generating texts for topics or domains that are well-known or overrepresented in the training data.
- We can use a higher temperature when generating texts for users who prefer novelty and variety; then lower it when generating texts for users who prefer accuracy and consistency.


Temperature is an important hyperparameter that affects how large language models generate natural language texts. It controls how much randomness or uncertainty is introduced into the predictions by scaling the logits before applying softmax.

By adjusting temperature according to different purposes and scenarios, we can influence the quality and diversity of the generated texts.

#Prompting Basics

Prompting for LLMs is a rewarding and important skill that requires understanding the capabilities and limitations of the models, as well as the best practices and techniques for crafting effective prompts.

In this article, we will introduce the basics of prompting for LLMs, such as GPT-3, and discuss some of the key concepts and methods involved. We will also provide some examples and resources for further learning and experimentation.

What is prompting?

Prompting is a way of communicating with LLMs using natural language inputs that instruct them to perform a specific task or generate a specific output. A prompt typically consists of two parts: a task description and an example or template.

The task description is a natural language statement that defines the goal or objective of the task, such as "Translate from English to Spanish" or "Write a summary of the following article". The task description should be clear, concise, and unambiguous, and should avoid using jargon or technical terms that the model may not understand.

The example or template is a natural language snippet that demonstrates the expected format or structure of the output, such as "English: I like cats. Spanish: Me gustan los gatos." or "[Summary]: This article is about ...". The example or template should be consistent with the task description and should provide enough information for the model to infer the general pattern or logic of the output.

A prompt can be either few-shot or zero-shot, depending on whether it includes one or more examples or templates, or none at all. Few-shot prompts are more common and effective, as they provide more guidance and context for the model to follow. Zero-shot prompts are more challenging and require more creativity and skill, as they rely solely on the task description and the model's prior knowledge.

Why is prompting important?

Prompting is important because it directly affects the quality and usefulness of the outputs generated by LLMs. A well-designed prompt can elicit accurate and relevant responses from the model, while a poorly-designed prompt can result in irrelevant or nonsensical responses, or even trigger undesirable behaviors such as bias or toxicity.

Prompting is also important because it enables users to leverage the generalization and adaptation abilities of LLMs, without requiring extensive fine-tuning or retraining. By using different prompts, users can access different aspects or modes of the model's knowledge and skills, and apply them to various domains and tasks.

Prompting is also important because it reveals the strengths and weaknesses of LLMs, as well as their underlying assumptions and biases. By analyzing how different prompts affect the model's performance and behavior, users can gain insights into how LLMs work and what they can and cannot do.

How to prompt effectively?

Prompting effectively requires a combination of art and science, as well as trial and error. There is no single best way to prompt an LLM for a given task or domain, as different models may have different preferences and sensitivities. However, there are some general principles and techniques that can help users craft better prompts.

Some of these techniques include:

- Use low-level patterns from other examples to make a given prompt easier to understand for an LLM.
- Explictly itemize instructions into bulleted lists.
- When possible, break down a top-level task into different sub-tasks that can be executed in parallel or sequentially.
- Avoid repeated and generic statements when trying to solve a very specific task.
- Use narratives and cultural anchors to encode nuanced intentions.
- Encourage deconstruction of a problem into components before producing a verdict.
- Use constraints and control flow to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.
- Use metaprompts that seed the model to generate its own natural language prompts for a range of tasks.

Examples of prompting

To illustrate some of these techniques in action, let us look at some examples of prompting for different tasks using GPT-3 (via OpenAI Playground).

Task: Write a haiku about winter.

Zero-shot prompt:

Write a haiku about winter. A haiku is a short poem that consists of three lines with five, seven, and five syllables respectively.

Output:

Snowflakes gently fall
Covering the earth in white
Winter has arrived

Few-shot prompt:

Write a haiku about winter. A haiku is a short poem that consists of three lines with five, seven, and five syllables respectively. Here are some examples:

Cold wind blows fiercely
Trees are bare and branches creak
Winter solitude

The pond is frozen
Children skate with joy and laughter
Winter wonderland

Output:

Winter night is long
Stars shine brightly in the sky
A silent beauty

## Do’s and Don’ts of Prompting

Large language models (LLMs) such as ChatGPT are powerful tools that can generate natural language texts on various topics and tasks. However, to get the most out of these models, we need to know how to communicate with them effectively. This is where prompt engineering comes in.

Prompt engineering is the skill of crafting inputs or instructions for LLMs that elicit the desired outputs or behaviors. A good prompt can make the difference between a coherent and relevant response and a nonsensical or off-topic one. In this article, we will explore some general dos and don'ts of effective LLM/chatgpt prompting, based on existing research and best practices.

Do: Use keywords, phrases, and bullet points
One of the simplest ways to improve your prompts is to use keywords, phrases, and bullet points that capture the essence of your question or query. These can help the LLM focus on the most important aspects of your input and avoid irrelevant details or distractions. For example, instead of asking:

What are some good books to read about artificial intelligence?

You can use keywords and phrases like:

- Books
- Artificial intelligence
- Recommendations

Or bullet points like:

- Recommend books
- Topic: artificial intelligence
- Explain why

Don't: Use long or complex sentences
Long or complex sentences can confuse the LLM or make it harder for it to understand your intent. They can also introduce ambiguity or inconsistency that can affect the quality of the output. For example, instead of asking:

Can you please write a short summary of the main points of this article that I found online about how to use ChatGPT for prompt engineering?

You can use keywords and phrases like:

- Summary
- Article
- ChatGPT
- Prompt engineering

Or bullet points like:

- Write summary
- Source: article
- Topic: ChatGPT and prompt engineering
- Length: short

Do: Use examples and templates
Another way to improve your prompts is to use examples and templates that show the LLM what kind of output you expect. Examples can provide concrete guidance and feedback for the LLM, while templates can provide structure and format for the output. For example, instead of asking:

Can you write a poem about love?

You can use an example like:

Write a poem about love, like this one:

Roses are red
Violets are blue
Sugar is sweet
And so are you

Or a template like:

Write a poem about love, using this template:

[Something] are [color]
[Something] are [color]
[Something] is [adjective]
And so are [pronoun]

Don't: Use vague or open-ended prompts
Vague or open-ended prompts can lead to unpredictable or irrelevant outputs from the LLM. They can also make it harder for the LLM to infer your goal or criteria for a good output. For example, instead of asking:

Can you write something interesting?

You can use keywords and phrases like:

- Write
- Interesting
- Topic: space exploration

Or bullet points like:

- Write something interesting
- Choose a topic: space exploration
- Include facts and opinions

Do: Use follow-up questions and feedback
One of the advantages of using LLMs like ChatGPT is that they can maintain a conversational context and respond to follow-up questions and feedback. This can help you refine your prompts and outputs, as well as explore different aspects or perspectives of your topic or task. For example, after asking:

Write a summary of this article about ChatGPT prompt engineering.

You can use follow-up questions like:

- How long is the summary?
- What are the main points?
- How confident are you in the summary?

Or feedback like:

- The summary is too long, make it shorter.
- The summary is missing some important details, add them.
- The summary is accurate and concise, good job.

Don't: Use inconsistent or contradictory prompts
Inconsistent or contradictory prompts can confuse the LLM or make it impossible for it to generate a valid output. They can also undermine your credibility and trustworthiness as a prompter. For example, instead of asking:

Write a summary of this article about ChatGPT prompt engineering in 10 words or less.

You can use consistent and realistic prompts like:

Write a summary of this article about ChatGPT prompt engineering in one sentence.

Or

Write three keywords that summarize this article about ChatGPT prompt engineering.

#Zero Shot Prompting

Zero shot prompting is a technique that allows natural language models to perform tasks without any explicit training or fine-tuning. It relies on the ability of the models to use natural language as a general interface for various tasks, such as text classification, sentiment analysis, question answering, and more. In this article, we will explain what zero shot prompting is, how it works, and how to use it effectively.

What is zero shot prompting?

Zero shot prompting is based on the idea that natural language models can learn from large amounts of text data and encode general knowledge and linguistic skills that can be applied to different tasks. For example, a model that is trained on a large corpus of books, news articles, and web pages can learn about different topics, facts, opinions, and styles of writing. By using natural language as a prompt, we can ask the model to perform a specific task using its existing knowledge and skills.

A prompt is a natural language input that specifies the task and the desired output format. For example, if we want to use a model to classify a text into one of four categories: positive, negative, neutral, or mixed, we can use the following prompt:

Text: I love this movie. It was so funny and entertaining.
Sentiment:

A possible output for this prompt could be:

Sentiment: positive

The output is the model's prediction based on the prompt and the text. The prompt can be seen as a way of instructing the model what to do and how to do it. The model does not need any additional training or fine-tuning for this task. It can use its existing knowledge and skills to generate an output that matches the prompt.

How does zero shot prompting work?

Zero shot prompting works by leveraging the pre-trained weights and architecture of natural language models. These models are usually trained on large amounts of text data using self-supervised learning objectives, such as masked language modeling or next sentence prediction. These objectives encourage the models to learn how to fill in missing words or sentences in a given context, which requires them to capture various aspects of natural language, such as syntax, semantics, pragmatics, and common sense.

By using natural language as a prompt, we can exploit the model's ability to generate coherent and relevant outputs that match the context. The model can use its pre-trained weights and architecture to encode the prompt and the text into a high-dimensional vector representation. Then, it can use its decoder or output layer to generate an output that maximizes the probability of being correct given the input.

Depending on the task and the output format, different methods can be used to generate the output. For example, for text classification tasks, where the output is a discrete label or category, we can use a softmax layer or a logit layer to compute the probability distribution over the possible labels and select the most likely one. For text generation tasks, where the output is a natural language text, we can use beam search or greedy decoding to generate words or tokens one by one until a stop condition is met.

How to use zero shot prompting effectively?

Zero shot prompting is a powerful technique that can enable natural language models to perform various tasks without any explicit training or fine-tuning. However, it also has some limitations and challenges that need to be considered. Here are some tips on how to use zero shot prompting effectively:

- Choose an appropriate model: Different models have different capabilities and limitations depending on their size, architecture, training data, and objectives. For example, some models may be better at capturing factual knowledge than others. Some models may be more fluent or diverse than others. Some models may have biases or errors that affect their outputs. Therefore, it is important to choose a model that suits your task and your expectations.
- Design a clear and consistent prompt: The prompt is the key element that defines the task and guides the model's output. Therefore, it is important to design a prompt that is clear and consistent with your task and your output format. For example, you should avoid ambiguous or vague prompts that may confuse the model or lead to unexpected outputs. You should also avoid prompts that are too long or too complex that may overload the model's memory or attention. You should also try to use prompts that are similar or compatible with the model's training data or objectives.
- Evaluate and refine your outputs: Zero shot prompting does not guarantee that the model's outputs will be correct or satisfactory for your task. Therefore, it is important to evaluate and refine your outputs using various methods and criteria. For example, you can use human evaluation or automatic metrics to measure the quality, accuracy, relevance, diversity, or coherence of your outputs. You can also use feedback or reinforcement learning to improve your outputs over time. You can also use data augmentation or adversarial examples to test your outputs against different scenarios or inputs.

Some good examples of zero shot prompts and answers are:

- If John has 5 pears, then eats 2, and buys 5 more, then gives 3 to his friend, how many pears does he have? Let's think step by step. 
  - Answer: John starts with 5 pears. He eats 2 pears, leaving him with 3 pears. He buys 5 more pears, giving him a total of 8 pears. He gives 3 pears to his friend, leaving him with only 5 pears.
- Write a story about a frog and a mushroom who become friends. Let's think step by step. 
  - Answer: A frog was hopping through the forest when he saw a mushroom growing under a tree. He was curious and decided to talk to it. "Hello, mushroom. What are you doing here?" he asked. The mushroom was surprised and happy to have a visitor. "Hello, frog. I'm just enjoying the shade and the moisture. Do you want to join me?" it said. The frog agreed and sat next to the mushroom. They started to chat and found out that they had a lot in common. They both liked the rain, the smell of the earth, and the sound of the birds. They became friends and decided to meet again the next day.
- TL;DR: Putting the letters TL;DR: after a block of text will usually give you a summary. A simple one-word prompt for summarization 
  - Answer: TL;DR: How to use TL;DR for summarization

## Few Shot Prompting

Few-shot prompting is a technique used in natural language processing to create text with a limited amount of input data. It is based on the idea that large language models, such as GPT-3, can learn from a small number of examples, typically between two and five, to quickly adapt to new tasks or domains. In this article, we will explain the basics of few-shot prompting and how it works.

Few-shot prompting consists of three main components: a task description, examples, and a prompt. The task description is a natural language statement that defines the goal of the task, such as "write a summary of the following article" or "classify the sentiment of the following review". The examples are pairs of input and output that demonstrate how the task should be performed, such as "Article: Scientists discover a new species of frog in the Amazon rainforest. Summary: A new frog species with bright colors and a distinctive call has been found in the Amazon." or "Review: This product is amazing! I love it! Sentiment: Positive". The prompt is the input for which we want the model to generate an output, such as "Article: A new study shows that chocolate can improve memory and mood. Summary:" or "Review: I hate this product! It broke after one use! Sentiment:".

The idea behind few-shot prompting is that by providing the model with a task description and examples, we can condition it to perform the task on new inputs. The model can use its general knowledge and language skills to infer the rules and patterns of the task from the examples, and apply them to the prompt. For example, if we give the model a task description and examples of summarizing articles, it can learn to extract the main points and write them in a concise way. If we give it a task description and examples of classifying sentiments, it can learn to identify positive and negative words and phrases.

Few-shot prompting is an effective way to leverage the power of large language models without requiring much data or fine-tuning. It can also be used for tasks that are not well-defined or standardized, such as writing creative texts or generating questions. However, few-shot prompting also has some limitations and challenges. For instance, it may not work well for tasks that require complex reasoning or domain-specific knowledge, such as solving math problems or diagnosing diseases. It may also be sensitive to the quality and quantity of the examples, as well as the wording and format of the task description and prompt. Moreover, it may generate outputs that are inaccurate, inconsistent, or biased, depending on the model's training data and capabilities.

In conclusion, few-shot prompting is a promising technique for natural language processing that allows us to use large language models for various tasks with minimal data. It works by providing the model with a task description and examples that condition it to generate an output for a given prompt. However, it also has some limitations and challenges that need to be addressed and overcome.

## Chain of Thought Prompting

Chain of thought LLM prompting is a technique for generating natural language texts using large language models (LLMs) such as GPT-3. It involves providing a sequence of prompts that guide the LLM to produce coherent and relevant texts on a given topic. The prompts are designed to elicit specific responses from the LLM that form a logical chain of thought. The prompts can be questions, statements, keywords, or any other textual cues that the LLM can understand and respond to.

The main advantage of chain of thought LLM prompting is that it allows the user to control the direction and quality of the generated texts. By carefully choosing the prompts, the user can steer the LLM to cover the desired aspects of the topic, avoid repetition or inconsistency, and generate informative and engaging texts. The prompts can also help the LLM to overcome its limitations, such as lack of factual knowledge, common sense, or domain expertise.

To illustrate how chain of thought LLM prompting works, let us consider an example of generating a blog post on the benefits of meditation. The user can start with a simple prompt such as "What is meditation?" and then follow up with more specific prompts that explore different aspects of meditation, such as its history, types, effects, and tips. Here is a possible sequence of prompts and responses:

- What is meditation?
- Meditation is a practice of focusing one's attention on a single object, thought, or sensation, such as breathing, a mantra, or a candle flame. It is usually done in a quiet and comfortable environment, with eyes closed or partially open. Meditation can help to calm the mind, reduce stress, and enhance well-being.
- How long has meditation been practiced?
- Meditation has been practiced for thousands of years in various cultures and religions. It is believed to have originated in India around the 6th century BCE as part of Hinduism and Buddhism. Later, it spread to other parts of Asia, such as China, Japan, and Tibet. In the West, meditation became popular in the 20th century through the influence of teachers such as Maharishi Mahesh Yogi, Thich Nhat Hanh, and Jon Kabat-Zinn.
- What are some types of meditation?
- There are many types of meditation, each with its own purpose and technique. Some of the most common ones are:

  - Mindfulness meditation: This involves observing one's thoughts and feelings without judging or reacting to them. It helps to cultivate awareness and acceptance of the present moment.
  - Transcendental meditation: This involves repeating a specific sound or word (called a mantra) silently in one's mind. It helps to achieve a state of deep relaxation and inner peace.
  - Zen meditation: This involves sitting in a specific posture (called zazen) and focusing on one's breathing or counting breaths. It helps to develop concentration and insight into the nature of reality.
  - Loving-kindness meditation: This involves sending positive thoughts and emotions (such as compassion, kindness, and gratitude) to oneself and others. It helps to foster empathy and happiness.
- What are some effects of meditation?
- Meditation can have various effects on one's physical, mental, and emotional health. Some of the benefits are:

  - Reduced stress and anxiety: Meditation can lower the levels of cortisol (a stress hormone) and increase the levels of serotonin (a mood-enhancing hormone) in the body. It can also reduce the activity of the amygdala (a brain region involved in fear and anger) and increase the activity of the prefrontal cortex (a brain region involved in rational thinking and decision making).
  - Improved memory and attention: Meditation can enhance one's cognitive abilities by strengthening the connections between brain cells and increasing the size of brain regions related to learning and memory. It can also improve one's focus and concentration by filtering out distractions and irrelevant information.
  - Enhanced immune system: Meditation can boost one's immune system by stimulating the production of antibodies (proteins that fight infections) and reducing inflammation (a response to injury or illness). It can also lower blood pressure and heart rate by relaxing the blood vessels and muscles.
  - Increased happiness and well-being: Meditation can increase one's happiness and well-being by fostering positive emotions (such as joy, love, and satisfaction) and reducing negative emotions (such as sadness, anger, and fear). It can also improve one's self-esteem and confidence by developing a more balanced and realistic view of oneself and others.
- What are some tips for practicing meditation?
- Some tips for practicing meditation are:

  - Choose a suitable time and place: It is advisable to meditate at a regular time (preferably in the morning or evening) when one is not too tired or busy. It is also important to find a quiet and comfortable place where one will not be disturbed or distracted by noise or people.
  - Start with small sessions: It is recommended to start with short sessions (such as 5 or 10 minutes) and gradually increase the duration (up to 20 or 30 minutes) as one becomes more familiar and comfortable with meditation. It is better to meditate for a few minutes every day than for a long time once in a while.
  - Be patient and gentle: It is normal to encounter difficulties or challenges when meditating, such as wandering thoughts, physical discomforts, or emotional reactions. It is important to be patient and gentle with oneself and not get frustrated or discouraged by these obstacles. Instead, one should acknowledge them without judgment or attachment and gently bring one's attention back to the chosen object or technique.
  - Experiment with different methods: There is no one right way to meditate. Different methods may suit different people depending on their preferences, goals, personalities, and backgrounds. One should experiment with different types of meditation until one finds what works best for oneself.

This is an example of how chain of thought LLM prompting can generate a blog post on the benefits of meditation using a series of prompts that elicit informative
and relevant responses from an LLM. Of course, this is not the only possible way to generate such a text; there may be other ways to choose different prompts or arrange them differently. The key point is that chain of thought LLM prompting allows the user to have more control over the content
and quality of the generated texts by providing appropriate guidance
and feedback to the LLM.

## Self Consistency

Self-Consistency: A New Decoding Strategy for Chain-of-Thought Reasoning

Chain-of-thought (CoT) prompting is a technique that leverages pre-trained large language models (LMs) to perform complex reasoning tasks by breaking them down into a series of simpler sub-questions and answers. For example, given the question "What is the capital of the country that borders France and Germany?", a CoT prompt could be:

Q: What is the name of the country that borders France and Germany?
A: Belgium
Q: What is the capital of Belgium?
A: Brussels

The answer to the original question is then the final answer of the CoT prompt, which is Brussels. CoT prompting has shown promising results on various arithmetic and commonsense reasoning benchmarks, such as GSM8K, SVAMP, AQuA, StrategyQA and ARC-challenge.

However, CoT prompting also has some limitations. One of them is that it relies on naive greedy decoding to generate each sub-question and answer, which can lead to errors or inconsistencies. For example, given the question "How many minutes are in 5 hours?", a CoT prompt could be:

Q: How many hours are in a day?
A: 24
Q: How many minutes are in an hour?
A: 60
Q: How many minutes are in 5 hours?
A: 300

The answer to the original question is correct, but the first sub-question and answer are irrelevant and unnecessary. Moreover, greedy decoding can also produce wrong or contradictory answers due to noise or ambiguity in the input or the LM. For example, given the question "Who is the president of the United States in 2023?", a CoT prompt could be:

Q: What year is it now?
A: 2023
Q: Who won the 2020 US presidential election?
A: Joe Biden
Q: Who is the president of the United States now?
A: Donald Trump

The answer to the original question is wrong and inconsistent with the previous sub-answer. This shows that greedy decoding can fail to capture the logical coherence and consistency of a complex reasoning problem.

To address this issue, Wang et al. (2022) proposed a new decoding strategy called self-consistency, which aims to improve CoT reasoning by sampling multiple diverse reasoning paths and selecting the most consistent answer among them. The intuition behind self-consistency is that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. For example, given the question "Who is the president of the United States in 2023?", some possible reasoning paths are:

Path 1:
Q: What year is it now?
A: 2023
Q: Who won the 2020 US presidential election?
A: Joe Biden
Q: Who is the president of the United States now?
A: Joe Biden

Path 2:
Q: How long is a US presidential term?
A: 4 years
Q: When did Joe Biden take office as president?
A: January 20, 2021
Q: When will his term end?
A: January 20, 2025
Q: Who is the president of the United States now?
A: Joe Biden

Path 3:
Q: Who was the vice president of Barack Obama?
A: Joe Biden
Q: Who ran for president in 2020 as a Democrat?
A: Joe Biden
Q: Who won the 2020 US presidential election?
A: Joe Biden
Q: Who is the president of the United States now?
A: Joe Biden

All three paths lead to the same correct answer, which is Joe Biden. Self-consistency leverages this property by sampling a diverse set of reasoning paths using beam search or top-k sampling, and then selecting the most consistent answer by marginalizing out the sampled paths. In other words, self-consistency chooses the answer that appears most frequently among all the sampled paths. For example, if we sample 10 paths for the question above, and 8 of them have Joe Biden as the final answer, while 2 of them have Donald Trump or Kamala Harris as the final answer, self-consistency will choose Joe Biden as the most consistent answer.

Self-consistency has several advantages over greedy decoding for CoT prompting. First, it can reduce errors and inconsistencies by averaging out noise and ambiguity from individual paths. Second, it can increase diversity and creativity by exploring different ways of thinking about a problem. Third, it can enhance robustness and generalization by adapting to different inputs and tasks.

Wang et al. (2022) evaluated self-consistency on a range of popular arithmetic and commonsense reasoning benchmarks, and showed that it significantly improved the performance of CoT prompting with a striking margin. For example, on GSM8K, self-consistency achieved an accuracy of 87.9%, compared to 70% for greedy decoding; on SVAMP, self-consistency achieved an accuracy of 81%, compared to 70% for greedy decoding; on AQuA, self-consistency achieved an accuracy of 72.2%, compared to 60% for greedy decoding; on StrategyQA, self-consistency achieved an accuracy of 56.4%, compared to 50% for greedy decoding; and on ARC-challenge, self-consistency achieved an accuracy of 43.9%, compared to 40% for greedy decoding.

In conclusion, self-consistency is a new decoding strategy that improves CoT reasoning by sampling multiple diverse reasoning paths and selecting the most consistent answer among them. It leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. It has shown remarkable results on various arithmetic and commonsense reasoning benchmarks, and has opened up new possibilities for prompt engineering with pre-trained LMs.


## Generated Knowledge Prompting 

Generated Knowledge Prompting (GKP) is a novel method for enhancing commonsense reasoning with external knowledge, proposed by Liu et al. 2022. In this article, we will explain the key specifics of GKP and how it works.

Commonsense reasoning is the ability to make sense of everyday situations and infer implicit information that is not explicitly stated. For example, given the question "What is the most likely reason that someone would cut their own hair?", a commonsense reasoner should be able to answer "To save money" or "To change their appearance", among other possible reasons.

However, commonsense reasoning is challenging for natural language processing (NLP) systems, especially when the question requires some background knowledge that is not provided in the context. For instance, to answer the question "What is the most likely reason that someone would put a wet towel on their forehead?", a system would need to know that a wet towel can help reduce fever or headache.

One way to address this challenge is to use external knowledge sources, such as structured knowledge bases (KBs) or unstructured text corpora, to retrieve relevant facts and provide them as additional input to the NLP system. However, this approach has some limitations:

- It requires access to a large and comprehensive KB or corpus, which may not exist for some domains or topics.
- It requires a reliable and efficient retrieval mechanism, which may not be able to find all the relevant facts or filter out the irrelevant ones.
- It requires a way to integrate the retrieved facts with the original context and question, which may not be trivial or consistent across different tasks.

Another way to address this challenge is to leverage the inherent knowledge stored in large-scale pretrained language models (LLMs), such as GPT-3 or T5, which have been shown to encode a vast amount of factual and commonsense knowledge in their parameters. However, this approach also has some limitations:

- It relies on the LLM's ability to recall and apply the relevant knowledge given a suitable prompt, which may not be easy or robust for some questions or tasks.
- It does not allow for explicit control or inspection of the knowledge used by the LLM, which may lead to errors or biases.

GKP aims to overcome these limitations by combining the advantages of both approaches. GKP consists of two steps: knowledge generation and answer prediction.

Knowledge Generation

In the knowledge generation step, the LLM is asked to generate a set of facts about the question. The LLM is prompted in few-shot fashion as seen below. M different completions are generated using this same prompt (similar to the self-consistency approach).

Q: What is something that people usually do before they go to bed?
Facts:
1. They brush their teeth.
2. They change into pajamas.
3. They read a book or watch TV.
4. They set an alarm clock.
5. They turn off the lights.

The generated facts are then concatenated with the original question and context as additional input for the answer prediction step.

Answer Prediction

In the answer prediction step, the LLM is asked to predict the answer based on the question, context, and generated facts. The LLM is prompted in few-shot fashion again as seen below. The final answer is selected based on a scoring function that considers various factors, such as log-probability, self-consistency, and diversity.

Q: What is something that people usually do before they go to bed?
Facts:
1. They brush their teeth.
2. They change into pajamas.
3. They read a book or watch TV.
4. They set an alarm clock.
5. They turn off the lights.
A: One possible answer is: They read a book or watch TV.

GKP has several advantages over existing methods:

- It does not require access to a KB or corpus, but rather uses the LLM itself as a flexible and dynamic source of knowledge.
- It does not require a retrieval mechanism, but rather uses the LLM's generation ability to produce relevant facts on demand.
- It does not require task-specific supervision or fine-tuning for knowledge integration, but rather uses few-shot prompting to guide the LLM's reasoning process.
- It allows for explicit control and inspection of the knowledge used by the LLM, which may help improve its accuracy and explainability.

Liu et al. 2022 evaluated GKP on four commonsense reasoning tasks: NumerSense (numerical commonsense), CommonsenseQA 2.0 (general commonsense), QASC (scientific commonsense), and COPA (causal commonsense). They showed that GKP improves the performance of state-of-the-art LLMs on all four tasks, achieving new state-of-the-art results on three of them. They also conducted extensive ablation studies and analysis to demonstrate the effectiveness and robustness of GKP.

In conclusion, GKP is a promising method for enhancing commonsense reasoning with external knowledge, using only a single pretrained LLM and few-shot prompting. GKP highlights the potential of LLMs as flexible sources of knowledge for improving NLP systems.

References

: Liu et al. 2022. Generated Knowledge Prompting for Commonsense Reasoning. In Proceedings of ACL 2022.

## Automated Prompt Engineering

In this article, I will explain the key specifics of automated prompt engineering and how it works as proposed by Zhou et al., (2022) with examples. Automated prompt engineering is a technique that aims to generate natural language instructions that can guide large language models (LLMs) to perform various tasks without fine-tuning or human intervention. Zhou et al., (2022) propose a framework called Automatic Prompt Engineer (APE) that leverages LLMs to generate and select the best instruction for a given task and output demonstration.

The main idea behind APE is to treat the instruction as a "program" that can be optimized by searching over a pool of instruction candidates proposed by an LLM. The instruction candidates are generated by conditioning an LLM on the task description and some output examples. For instance, if the task is to write a summary of a news article, the LLM can be given the article title and some summary sentences as output examples. The LLM will then generate a set of possible instructions that can be used to prompt another LLM to write a summary.

The instruction candidates are then evaluated by executing them on a target LLM and measuring their performance using a score function. The score function can be based on different criteria, such as accuracy, fluency, diversity, etc. The instruction candidate with the highest score is selected as the final instruction for the task. Zhou et al., (2022) show that APE can generate instructions that outperform or match human-engineered instructions on 19 out of 24 NLP tasks.

One of the advantages of APE is that it can generate instructions that are tailored to the specific task and output demonstration, rather than relying on generic or predefined prompts. For example, for the task of generating chain-of-thought reasoning for arithmetic problems, APE generates an instruction that says "Let's work this out in a step by step way to be sure we have the right answer." This instruction elicits more detailed and accurate reasoning than the human-engineered instruction "Let's think step by step" (Kojima et al., 2022).

Another advantage of APE is that it can generate instructions that can steer LLMs toward desired properties, such as truthfulness or informativeness. For example, for the task of generating headlines for news articles, APE generates an instruction that says "Write a headline that summarizes the main point of the article and does not mislead the reader." This instruction leads to more factual and informative headlines than the human-engineered instruction "Write a headline for this article" (Zhou et al., 2022).

In conclusion, automated prompt engineering is a promising technique that can harness the power of LLMs for various tasks without requiring fine-tuning or human expertise. APE is a framework that automates the instruction generation and selection process using LLMs themselves. APE can generate effective and adaptable instructions that can improve the performance and quality of LLM outputs.

## Directional Stimulus

Directional Stimulus Prompting: A New Way to Guide Large Language Models

Large language models (LLMs) such as GPT-3 and Codex have shown impressive capabilities in generating natural language texts for various tasks, such as summarization and dialogue response generation. However, these models are often trained on massive amounts of data that may not be relevant or aligned with the specific task or domain. Moreover, these models are usually frozen and black-boxed, meaning that they cannot be fine-tuned or modified for different tasks.

How can we better guide LLMs to generate the desired texts without changing their parameters? One common approach is to use prompts, which are carefully crafted inputs that elicit the expected outputs from the LLMs. For example, to summarize an article, we can prepend a phrase like "TL;DR:" or "In a nutshell:" to the article text and feed it to the LLM. However, finding the optimal prompt for each task can be challenging and time-consuming, and may require manual or automatic search over a large space of possible prompts.

Li et al., (2023) propose a new framework, called Directional Stimulus Prompting (DSP), that uses a tuneable language model (LM) to provide guidance for the black-box frozen LLM on downstream tasks. Unlike prior work that uses fixed prompts, DSP trains a policy LM to generate discrete tokens as directional stimulus of each input, which is a hint or cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target.

The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, Li et al., (2023) apply their framework to summarization and dialogue response generation tasks. Experimental results demonstrate that it can significantly improve LLMs' performance with a small collection of training data: a T5 (780M) trained with 2,000 samples from the CNN/Daily Mail dataset improves Codex (175B)'s performance by 9.0% in ROUGE-Avg scores; only 80 dialogues can boost the combined score by 39.7%, achieving comparable or even better performance than some fully trained models on the MultiWOZ dataset.

DSP is a novel and promising technique that leverages a small and tuneable policy LM to generate hints that guide a large and frozen LLM. This way, DSP can adapt LLMs to different tasks and domains without changing their parameters or requiring large amounts of data. DSP also opens up new possibilities for exploring more effective and diverse ways of providing stimulus for LLMs.

# Use Cases

## Code Generation

Comments to Code

Sure, here's an example of how to turn those comments into Python code.

```python
# 1. Create a list of games
games = ['Game1', 'Game2', 'Game3', 'Game4', 'Game5', 'Game6', 'Game7', 'Game8', 'Game9', 'Game10']

# 2. Create a list of ratings for these games
ratings = [4.5, 3.7, 4.2, 5.0, 3.8, 4.1, 4.9, 3.2, 4.4, 4.7]

# 3. Combine them to make a json object of 10 games with their ratings
import json

# We'll create a dictionary where the keys are game names and the values are their ratings
games_with_ratings = dict(zip(games, ratings))

# Now let's convert the dictionary to a json object
games_with_ratings_json = json.dumps(games_with_ratings)

print(games_with_ratings_json)
```

When you run this code, it will print out a JSON object where each game name is paired with its rating, looking something like this:

```json
{"Game1": 4.5, "Game2": 3.7, "Game3": 4.2, "Game4": 5.0, "Game5": 3.8, "Game6": 4.1, "Game7": 4.9, "Game8": 3.2, "Game9": 4.4, "Game10": 4.7}
```
This is a very basic example and might not cover all the intricacies of your specific use case, but it should give you a general idea of how to turn comments into code.






